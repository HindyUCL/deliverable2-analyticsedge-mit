---
title: "Problem_4"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
  html_notebook: default
header-includes:
  - \usepackage{fvextra}
  - \fvset{breaklines=true, breakanywhere=true} % wrap in all FancyVerb blocks
  - \DefineVerbatimEnvironment{verbatim}{Verbatim}{breaklines,breakanywhere}
---

# Problem 4: In this problem, you will compare three predictive analytics methods: linear regression, CART, and random forest.

```{r}
set.seed(15072)

library(dplyr)
library(caret)
library(rpart)  
library(randomForest)
library(tidyverse)
library(readr)
library(ggplot2)
library(olsrr)
library(rpart.plot)

df <- read_csv("./ames.csv")
head(df)
```

```{r}
# Split the data into test and train (70% training, 30% testing)
trainIndex <- createDataPartition(df$SalePrice, p = 0.7, list = FALSE)

df_train <- df[trainIndex, ]
df_test  <- df[-trainIndex, ]

cat(dim(df_train), "\n")
cat(dim(df_test))
```

## a)

```{r, warning=FALSE, message=FALSE}
# initial linear model
mod_linear_initial <- lm(SalePrice ~ ., data = df_train)

# backward elimination by p-value 
mod_linear_olsrr <- ols_step_backward_p(mod_linear_initial, p_val = 0.05, progress = TRUE)

# final model object returned by olsrr
mod_linear_final <- mod_linear_olsrr$model
summary(mod_linear_final)
```

```{r, warning=FALSE, message=FALSE}

R2 <- function(model, train, test) {
  
  # Predictions
  pred_train <- predict(model, newdata = train)
  pred_test  <- predict(model, newdata = test)
  
  # In-Sample R²
  SSE_train <- sum((train$SalePrice - pred_train)^2)
  SST_train <- sum((train$SalePrice - mean(train$SalePrice))^2)
  R2_train  <- 1 - SSE_train/SST_train
  
  # Out-of-Sample R²
  SSE_test <- sum((test$SalePrice - pred_test)^2)
  SST_test <- sum((test$SalePrice - mean(test$SalePrice))^2)
  R2_test  <- 1 - SSE_test/SST_test
  
  cat("In-sample R²: ", R2_train, "\n")
  cat("Out-of-sample R²: ", R2_test, "\n")
}

R2(mod_linear_final, df_train, df_test)
```

Out-of-Sample R-squared (0.795057) \< In-Samlpe R-squared (0.9166433), which is expected.

## b)

```{r}
# Train a CART model with default parameters
mod_cart <- rpart(SalePrice ~ ., data = df_train)

# Plot 
rpart.plot(mod_cart, main = "CART (default) — Ames Housing")

cat("\nTop variables by CART importance:\n")
print(sort(mod_cart$variable.importance, decreasing = TRUE)[1:10])

R2(mod_cart, df_train, df_test)
```

Out-of-Sample R-squared (0.7197951) \< In-Samlpe R-squared (0.7730154), which is expected. However, the difference between the two is smaller than what it was for our mod_linear_olsrr model, which exhibited higher Out-of-Sample R-squared (0.795057).

Let us visualize the variables' importance by decreasing order of importance from the CART model we trained.

```{r}
# Convert importance to data frame
var_imp <- data.frame(
  Variable = names(mod_cart$variable.importance),
  Importance = mod_cart$variable.importance
)

# Order by importance
var_imp <- var_imp %>%
  arrange(desc(Importance))

# Bar plot
ggplot(var_imp, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    title = "CART Variable Importance",
    x = "Variable",
    y = "Importance"
  ) +
  theme_minimal()
```

As we can see, the variables that influence the most variation in SalePrice are ExternalQual, Neighborhood, YearBuilt, KitchenQual, Garage, and YrBlt.

## c) 

Let's test the marginal effect of CentralAir using both models. We'll take an average counterfactual approach: - Pick homes in the test set that currently have no A/C - Predict their value as-is - Predict again after switching CentralAir = "Y" - Compare average lift

```{r}
# Ensure factor consistency
df_test$CentralAir <- factor(df_test$CentralAir, levels = c("N","Y"))

# Subset homes without A/C
no_ac <- df_test %>% filter(CentralAir == "N")

# Predictions with Linear Regression (part a)
pred_no_lin  <- predict(mod_linear_final, newdata = no_ac)
no_ac_yes    <- no_ac; no_ac_yes$CentralAir <- factor("Y", levels = c("N","Y"))
pred_yes_lin <- predict(mod_linear_final, newdata = no_ac_yes)
lift_lin <- mean(pred_yes_lin - pred_no_lin, na.rm = TRUE)

# Predictions with CART (part b)
pred_no_cart  <- predict(mod_cart, newdata = no_ac)
pred_yes_cart <- predict(mod_cart, newdata = no_ac_yes)
lift_cart <- mean(pred_yes_cart - pred_no_cart, na.rm = TRUE)

cat("Estimated lift from Central A/C (Linear Regression): $", round(lift_lin,2), "\n")
cat("Estimated lift from Central A/C (CART): $", round(lift_cart,2), "\n")

if (lift_lin > 15000) {
  cat("Linear Regression suggests: INSTALL A/C\n")
} else {
  cat("Linear Regression suggests: DO NOT install A/C\n")
}

if (lift_cart > 15000) {
  cat("CART suggests: INSTALL A/C\n")
} else {
  cat("CART suggests: DO NOT install A/C\n")
}
```

Results from both the linear regression and CART models suggest that adding central air conditioning would not generate a significant increase in the property’s sale price. Given that the estimated value uplift is essentially zero—well below the installation cost of \$15,000—we would advise against making this investment. This outcome also highlights that, in the context of our models, CentralAir is not a strong determinant of price once other influential features such as neighborhood, exterior quality, and year built are taken into account. Of course, she can instead over some of these important features, such as exterior quality by performing some rennovations, for instance, which we estimate will increase the SalePrice of her house. Our models have hence allowed us to advise her on how to invest these \$15,000 better to maximize SalesPrice.

## d)

```{r}
# Candidate cp values
cp_values <- c(5e-6, 5e-5, 5e-4, 5e-3, 5e-2)

# Perform 10-fold CV for each cp
cv_results <- data.frame(cp = cp_values, xerror = NA)

for (i in seq_along(cp_values)) {
  mod <- rpart(SalePrice ~ ., data = df_train,
               method = "anova",
               control = rpart.control(cp = cp_values[i], xval = 10))
  
  # xerror from cross-validation
  cv_results$xerror[i] <- mod$cptable[which.min(mod$cptable[,"xerror"]),"xerror"]
}

# Select cp with smallest cross-validated error
best_cp <- cv_results$cp[which.min(cv_results$xerror)]
cat("Best cp selected:", best_cp, "\n")

# Fit final CART model with best cp
mod_cart_final <- rpart(SalePrice ~ ., data = df_train,
                        method = "anova",
                        control = rpart.control(cp = best_cp))

# Report in-sample and out-of-sample R²
R2(mod_cart_final, df_train, df_test)
```

As we can see, selecting the best cp hyper paramter from the set of five explored through grid search significantly increases our Out-of-sample R² to 0.7701036 instead 0.7197951, gaining more than 5% points with the tuning.

## e)

```{r}
# Tune mtry using 10-fold CV
tune_grid <- expand.grid(mtry = 1:(ncol(df_train) - 1))

ctrl <- trainControl(method = "cv", number = 10)

rf_tuned <- train(
  SalePrice ~ .,
  data = df_train,
  method = "rf",
  trControl = ctrl,
  tuneGrid = tune_grid,
  ntree = 80,
  nodesize = 25
)

# Best mtry selected
best_mtry <- rf_tuned$bestTune$mtry
cat("Best mtry selected:", best_mtry, "\n")

# Fit final Random Forest with best mtry
mod_rf <- randomForest(
  SalePrice ~ .,
  data = df_train,
  ntree = 80,
  mtry = best_mtry,
  nodesize = 25,
  importance = TRUE
)

# Reports R² with our customed function
R2(mod_rf, df_train, df_test)
```

## f)

Let us calculate the number of weights to nuance our model choice.

```{r}
n_coef_lm <- length(coef(mod_linear_final)[!is.na(coef(mod_linear_final))])

fr_tuned    <- mod_cart_final$frame
cart_splits <- sum(fr_tuned$var != "<leaf>")
cart_leaves <- sum(fr_tuned$var == "<leaf>")
cart_params <- cart_splits + cart_leaves

rf_ntree        <- mod_rf$ntree
rf_nodes_per_t  <- sapply(seq_len(rf_ntree), function(k) nrow(getTree(mod_rf, k = k, labelVar = FALSE)))
rf_total_params <- sum(rf_nodes_per_t)
rf_avg_params   <- mean(rf_nodes_per_t)

cat("Linear Regression: ", n_coef_lm, "parameters (coefficients)\n")
cat("CART (tuned cp): ", cart_params, "parameters (", cart_splits, "splits +", cart_leaves, "leaves)\n")
cat("Random Forest: ~", round(rf_avg_params,1), "parameters per tree ×", rf_ntree, 
    "trees = ~", rf_total_params, "parameters total\n")
```

```{r, warning=FALSE, message=FALSE}
# Let's build a final comparision table to have clear vision on our models
r2_linear <- R2_values(mod_linear_final, df_train, df_test)
r2_cart   <- R2_values(mod_cart, df_train, df_test)
r2_cart_t <- R2_values(mod_cart_final, df_train, df_test)
r2_rf     <- R2_values(mod_rf, df_train, df_test)

# Make a simple comparison table
comparison <- data.frame(
  Model = c("Linear Regression (stepwise)",
            "CART (default)",
            "CART (tuned cp)",
            "Random Forest (tuned mtry)"),
  Hyperparameters = c("stepwise via olsrr (p = 0.05)",
                      paste0("cp = ", mod_cart$control$cp, 
                             ", minsplit = ", mod_cart$control$minsplit),
                      paste0("cp = ", best_cp, 
                             ", minsplit = ", mod_cart_final$control$minsplit),
                      paste0("mtry = ", best_mtry, 
                             ", ntree = ", mod_rf$ntree, 
                             ", nodesize = ", mod_rf$nodesize)),
  R2_in  = c(r2_linear["R2_in"], r2_cart["R2_in"], r2_cart_t["R2_in"], r2_rf["R2_in"]),
  R2_out = c(r2_linear["R2_out"], r2_cart["R2_out"], r2_cart_t["R2_out"], r2_rf["R2_out"])
)

# Display neatly
knitr::kable(comparison, digits = 4,
             caption = "Comparison of models: hyperparameters and performance")
```

Across all models, Random Forest with tuned hyperparameters (mtry = 50, ntree = 80, nodesize = 25) achieved the strongest predictive performance, with an in-sample R² of 0.9554 and an out-of-sample R² of 0.8610. This model used on average \~333.9 parameters per tree, resulting in about 26,710 total parameters across its 80 trees. By comparison, the stepwise linear regression model relied on just 115 coefficients, while the tuned CART tree used 147 parameters (73 internal splits + 74 leaves). Random Forest therefore operated with more than 230× as many weights as linear regression, and nearly 180× as many as CART, which explains its ability to capture highly nonlinear interactions.

The linear regression model nevertheless achieved competitive results (R²_out = 0.7951) while being two orders of magnitude more compact, and it offers interpretability via direct coefficients (e.g., the marginal effect of Central A/C). CART, with 147 parameters, strikes a middle ground but still lags behind both regression and Random Forest in accuracy (R²_out = 0.7198), suggesting that a single tree remains too simple for this dataset.

These parameter counts underscore the trade-off between accuracy and complexity: Random Forest achieves the best generalisation but at the cost of interpretability and computational overhead. Linear regression sacrifices some predictive power but provides direct, transparent insights, while CART offers an interpretable yet limited model. In practice, maintaining a portfolio of models is probably the best option: Random Forest when high predictive accuracy is required, and simpler models like regression for daily use cases where interpretability and efficiency matter most.

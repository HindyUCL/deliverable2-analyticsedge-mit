---
output:
  html_document:
    df_print: paged
  pdf_document:
    df_print: default
header-includes:
- \usepackage{fvextra}
- \RecustomVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,breakanywhere}
---

### R setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) 
```

```{r}
library(readr)
library(dplyr)
library(fitdistrplus)
library(ggplot2)

library(olsrr)
library(rpart)
library(rpart.plot)
library(caret)
library(randomForest)
```


# Problem 3
### Setup
```{r load df 3}
df_churn <- read_csv("/Users/riyaparikh_computeracct/Downloads/MIT/15.072_AdvancedAnalyticsEdge/deliverable2-analyticsedge-mit/customerchurn.csv", show_col_types = FALSE)
```

### Part a
```{r part a 3}
summary(df_churn)

barchartvector = c("Churn", "SeniorCitizen", "PaymentMethod", "InternetService", "Contract")
for (col_name in barchartvector) {
  counts <- table(df_churn[[col_name]])
  
  barplot(counts, 
          main = paste("Bar Chart for", col_name), 
          xlab = col_name, 
          ylab = "Frequency",
          col = "skyblue")
}

histvector = c("MonthlyCharges", "tenure")
for (col_name in histvector) {
  counts <- table(df_churn[[col_name]])
  
  hist(counts, 
          main = paste("Bar Chart for", col_name), 
          xlab = col_name, 
          ylab = "Frequency",
          col = "skyblue", breaks = "FD")
}

churnpercentage <- df_churn %>% filter(Churn == "1") %>% nrow() / length(df_churn$Churn)
nochurnpercentage <- 1-churnpercentage
cat("Churn percentage:", churnpercentage*100, "%", "and non churn percentage:", nochurnpercentage*100, "%")
```
From this output, we have a few key takeaways: the churn percentage is 26.5785% and the non churn percentage is 73.4215%, meaning we have approx 3x more observations of non churners than churners. It might be useful to balance the data set in following questions, we will see if necessary. We also realize that our data set is also unbalanced in terms of age demographics, having only 16.24% senior citizens. For the payment methods, users have the choice between 4 different payment methods including electronic check which is the most popular choice, and bank transfer/credit card/mailed check which all have pretty similar utilization. To provide more color on internet service, we see that the most popular service is fiber optic followed by DSL, but many people report not having a service. Moreover, the majority of people pay for these services month to month rather than being locked in for 1 or 2 years. With these services, we see that users pay on average of $64.80 per month, with a high of $118.75 and a low of $18.25. Charges are skewed right as is the histogram of tenure. 

### Part b
```{r part b 3}
set.seed(15072)

# creating a binary dependent variable for churn (0 = no, 1 = churn)
df_churn$Churn <- as.factor(df_churn$Churn)

# training (70%) and test (30%) partition
smp_size <- floor(0.70 * nrow(df_churn))
train_ind <- sample(seq_len(nrow(df_churn)), size = smp_size, replace = FALSE)
train_churn <- df_churn[train_ind, ]
test_churn <- df_churn[-train_ind, ]

#  exclude payment method from predictors in model
model <- glm(Churn ~ . - PaymentMethod, data = train_churn, family = "binomial")

summary(model)
```
The coefficient of 0.460054 for SeniorCitizen indicates that being a senior increases the log-odds of churn by 0.460054 (p < 0.001). In terms of odds, senior citizens are about 1.59 (=exp(0.460054)) times more likely to churn compared to non-seniors, holding all else constant.

### Part c
```{r part c 3}
fifth <- df_churn[5, ]

predictions <- predict(model, newdata = fifth)
print(predictions)
```
User is 73.06284% likely to churn.

### Part d
```{r part d 3}
predictionsfull <- predict(model, newdata = test_churn, type = "response")
custom_threshold <- 0.3
predicted_classes <- ifelse(predictionsfull > custom_threshold, 1, 0)

confusion_matrix <- table(predicted_classes, test_churn$Churn)
confusion_matrix
```

### Part e
False positive - we inaccurately predicted that a user will churn even though they did not actually.  
False negative - we inaccurately predicted that a user will not churn at the end of the year despite the fact that they did. 

### Part f
If I was a business analyst at Watson Analytics, I would focus on minimizing false negatives. Churn refers to the loss of customers, subscribers, or clients over a given period. It's a key metric for assessing customer satisfaction, loyalty, and the overall health of a business, especially in subscription-based models like we are looking at here. In the case of a false negative, we thought that a user would stay with us after the end of the year by assuming they were happy with our services. However, they in fact were unsatisfied and chose to leave us. This leaves us with a lost customer and an additional cost of having to expend time, money, and effort to gain new customers. While a false positive also isn't an accurate prediction from our model, its implications are far less harmful. It only means that a user ended up staying with our company despite the fact that we thought they'd leave. 

### Part g
```{r part g 3}
# increase threshold to 0.4
custom_threshold2 <- 0.4
predicted_classes2 <- ifelse(predictionsfull > custom_threshold2, 1, 0)
confusion_matrix2 <- table(predicted_classes2, test_churn$Churn)
dimnames(confusion_matrix2) <- list(Actual = c("No Churn", "Churn"), Pred = c("No Churn", "Churn")) 
confusion_matrix2 <- confusion_matrix2/sum(confusion_matrix2)

# increase threshold to 0.5
custom_threshold3 <- 0.5
predicted_classes3 <- ifelse(predictionsfull > custom_threshold3, 1, 0)
confusion_matrix3 <- table(predicted_classes3, test_churn$Churn)
dimnames(confusion_matrix3) <- list(Actual = c("No Churn", "Churn"), Pred = c("No Churn", "Churn")) 
confusion_matrix3 <- confusion_matrix3/sum(confusion_matrix3)

# increase threshold to 0.6
custom_threshold4 <- 0.6
predicted_classes4 <- ifelse(predictionsfull > custom_threshold4, 1, 0)
confusion_matrix4 <- table(predicted_classes4, test_churn$Churn)
dimnames(confusion_matrix4) <- list(Actual = c("No Churn", "Churn"), Pred = c("No Churn", "Churn")) 
confusion_matrix4 <- confusion_matrix4/sum(confusion_matrix4)

confusion_matrix_rate <- confusion_matrix/sum(confusion_matrix)

confusion_matrix_rate
confusion_matrix2
confusion_matrix3
confusion_matrix4

# rates
rates = seq(from = .3, to = .6, length.out = 4)

# false negatives
fnr = c(0.0563981, 0.09194313, 0.12322275, 0.15308057)

# false positives
fpr = c(0.2004739,0.12938389, 0.08815166, 0.05260664)

# plot multiple lines using matplot
matplot(rates, cbind(fnr, fpr), type = "l", lty = 1, 
        col = c("red", "blue"), xlab = "Thresholds", 
        ylab = "Rate", main = "Multiple Lines Plot")
legend("topright", legend = c("FNR Line", "FPR Line"), 
       col = c("red", "blue"), 
       lty = 1)
```
As you can see in the plot, as the threshold increases, the FPR increases and the FNR decreases. They both move in opposite directions. This is because the threshold signifies with what probability we must have confidence in our prediction for it to be classified "1". If this threshold keeps getting higher and higher, it is harder for us to have confidence of that high of a level. Thus, we label as "1" less often and the fpr goes down. 

### Part h
```{r part h 3}
thresholds <- seq(0, 1, length.out = 20)
total_value_list <- c()

for (i in thresholds) {
  predicted_classes <- ifelse(predictionsfull > i, 1, 0)
  confusion_matrix <- table(
  factor(predicted_classes, levels = c(0,1)),
  factor(test_churn$Churn, levels = c(0,1))
)
  
  # Payoff calculation
  TN <- confusion_matrix[1,1] * 3000
  FP <- confusion_matrix[2,1] * -1000
  FN <- confusion_matrix[1,2] * -6000
  TP <- confusion_matrix[2,2] * 2000
  
  total_val <- TN + FP + FN + TP
  total_value_list <- c(total_value_list, total_val)
}

# find best threshold
best_idx <- which.max(total_value_list)
best_threshold <- thresholds[best_idx]
best_profit <- total_value_list[best_idx]

cat("Best index: ", best_idx)
cat("Best threshold: ", best_threshold)
cat("Best profit: ", best_profit)
```

We'd optimize the probability threshold by finding out which threshold leads to max profit. By looping through 20 values from 0 to 1 (probability(Churn=1)=0 to probability(Churn=1)=1), we can see how each threshold produces different # FP, FN, TP, and TN Since each of these have an associated cost/profit with them, we can then multiply # * cost for all 4 result types. Then, by finding the one with the max profitability, we can backtrack and find which threshold was associated with that profitability.  
Here, the best threshold is 0.3684211 associated with a profit of $3,237,000.


















# Problem 4
### Setup
```{r load df}
df_ames <- read_csv("/Users/riyaparikh_computeracct/Downloads/MIT/15.072_AdvancedAnalyticsEdge/deliverable2-analyticsedge-mit/ames.csv", show_col_types = FALSE)

set.seed(15072)

# training (70%) and test (30%) partition
smp_size <- floor(0.70 * nrow(df_ames))
train_ind <- sample(seq_len(nrow(df_ames)), size = smp_size, replace = FALSE)
train_ames <- df_ames[train_ind, ]
test_ames <- df_ames[-train_ind, ]
```

### Part a
```{r part a 4}
mod_linear_intial <- lm(SalePrice ~ ., data = train_ames)
summary(mod_linear_intial)
mod_linear_olsrr <- ols_step_backward_p(mod_linear_intial, p_val = 0.05, progress = TRUE)
mod_linear_final <- mod_linear_olsrr$model
summary(mod_linear_final)

# out of sample Rsqd calc
out_of_sample_predictions <- predict(mod_linear_final, newdata = test_ames)

# calculate out-of-sample R-squared
out_of_sample_r_squared <- 1 - (sum((test_ames$SalePrice - out_of_sample_predictions)^2) /
                                sum((test_ames$SalePrice - mean(test_ames$SalePrice))^2))
out_of_sample_r_squared
```
For mod_linear_final:
In-sample R-squared (taken from outputted summary): 0.8983
Out-of-sample R-squared: 0.8254482

### Part b
```{r part b 4}
library(rpart)
modelcart = rpart(data = train_ames, SalePrice ~ .)
modelcart
library(rpart.plot)
prp(modelcart)
    
# in-sample predictions
in_sample_predictions <- predict(modelcart, newdata = train_ames)

# calculate in-sample R-squared
in_sample_r_squared <- 1 - (sum((train_ames$SalePrice - in_sample_predictions)^2) /
                            sum((train_ames$SalePrice - mean(train_ames$SalePrice))^2))

# out-of-sample predictions
out_of_sample_predictions <- predict(modelcart, newdata = test_ames)

# calculate out-of-sample R-squared
out_of_sample_r_squared <- 1 - (sum((test_ames$SalePrice - out_of_sample_predictions)^2) /
                                sum((test_ames$SalePrice - mean(test_ames$SalePrice))^2))

cat("In-sample R²:", round(in_sample_r_squared, 4), "\n")
cat("Out-of-sample R²:", round(out_of_sample_r_squared, 4), "\n")
```
```{r part b 4.1}
importance_scores <- modelcart$variable.importance
print(importance_scores)

sorted_importance <- sort(importance_scores, decreasing = TRUE)
print(sorted_importance)

library(ggplot2)
importance_df <- as.data.frame(sorted_importance)
importance_df$variable <- rownames(importance_df)
ggplot(importance_df, aes(x = reorder(variable, sorted_importance), y = sorted_importance)) +
  geom_col() +
  coord_flip() +
  labs(x = "Variable", y = "Importance", title = "Variable Importance in Regression Tree")
```

In sample R^2 is 0.7711155 and out of sample R^2 is 0.6909755. We have included our decision tree visualization above, and we can see from this plot of feature importances that the 5 most important variables are ExterQual, Neighborhood, KitchenQual, YearBuilt, and GarageYrBlt. This makes sense because the initial splits in our tree do occur based on ExterQua, GrLivAre, and GarageCa, and Neighbor shows up as well in following splits. These features must provide the most information gain in the tree, hence they are some of the most important features that tell us more about how to predict SalePrice based on other attributes provided.

### Part c
```{r part c 4}
coef(mod_linear_intial)["CentralAir"]
```
In order to see if it is worth it to have central air installed in order to increase the value of her home, we would want to compare the cost of installation ($15,000) to the predicted value added when a home has central air but all other factors remain constant. If the predicted value add is more than the cost, then yes it is worth it. Else, it is not going to be enough of a reason to spend the cost on the install. When we try to find the coefficient for "CentralAir" from our initial linear model, we get NA. When we try to see if "CentralAir" was one of the important features in our plot of Important Features in our decision tree, we see it doesn't show up. Meaning, "CentralAir" is not being valued in either of our models - the noninclusion suggests 0 value add. 
One thing I noticed was that, during the construction of models in part a, I was getting the error of aliased coefficients. I investigated further and found that a model has "aliased coefficients" when there is linear dependency among its predictor variables, meaning one or more predictors can be perfectly expressed as a linear combination of others, resulting in redundant information and making it impossible to uniquely estimate their coefficients. It indicates potential multicollinearity issues that require addressing by removing redundant variables or using other modeling techniques. However, we were told to assume no multicollinearity, which is obviously not the case in this dataset.


### Part d
```{r part d 4}
# defining the cp values to evaluate
cp_values <- c(5e-6, 5e-5, 5e-4, 5e-3, 5e-2)
tune_grid <- expand.grid(cp = cp_values)

# set up 10-fold cross-validation
ctrl <- trainControl(method = "cv", number = 10)

# perform cross-validation to find the best cp
set.seed(42)
tree_model_cv <- train(
  SalePrice ~ ., 
  data = train_ames, 
  method = "rpart",
  trControl = ctrl,
  tuneGrid = tune_grid
)

# result of cross validation
print(tree_model_cv)
```
```{r part d 4.1}
# reporting the best cp value selected by cross-validation based on lowest rmse
best_cp <- tree_model_cv$bestTune$cp
cat("The optimal cp value is:", best_cp, "\n")

# define the final model using the best cp
final_model <- rpart(SalePrice ~ ., data = train_ames, cp = best_cp)
print(final_model)

# in-sample R-squared
in_sample_pred <- predict(final_model, newdata = train_ames)
SST_in <- sum((train_ames$SalePrice - mean(train_ames$SalePrice))^2)
SSR_in <- sum((in_sample_pred - mean(train_ames$SalePrice))^2)
R2_in_sample <- SSR_in / SST_in
cat("In-sample R-squared:", R2_in_sample, "\n")

# out-of-sample R-squared from cross-validation results
out_of_sample_R2 <- max(tree_model_cv$results$Rsquared)
cat("Out-of-sample R-squared:", out_of_sample_R2, "\n")
```
RMSE was used to select the optimal model using the smallest value. The final value used for the model was cp = 5e-05 with n= 1972. This model has following stats: MAE of 21687.35, RMSE of 31979.45, in-sample R-squared: 0.9285921, and out-of-sample R-squared of 0.7904077. 

### Part e
```{r part e 4}
# The mtry parameter in a Random Forest model refers to the number of variables (or predictors) that are randomly sampled as candidates at each split when growing a decision tree within the forest

# cv control
control <- trainControl(method = "cv", number = 5)

# mtry tuning grid 
mtry_values <- data.frame(mtry = 1:73)

# train rf w 80 trees
rf_model_tuned <- train(
  SalePrice ~ .,
  data = train_ames,
  method = "rf",
  tuneGrid = mtry_values,
  trControl = control,
  ntree = 80,
  importance = TRUE
)

# display selected mtry
selected_mtry <- rf_model_tuned$bestTune$mtry
cat("Selected mtry value:", selected_mtry, "\n")

# insample R² 
pred_train <- predict(rf_model_tuned, newdata = train_ames)
SSE_train <- sum((train_ames$SalePrice - pred_train)^2)
SST_train <- sum((train_ames$SalePrice - mean(train_ames$SalePrice))^2)
R2_train <- 1 - SSE_train / SST_train

# oos R²
pred_test <- predict(rf_model_tuned, newdata = test_ames)
SSE_test <- sum((test_ames$SalePrice - pred_test)^2)
SST_test <- sum((test_ames$SalePrice - mean(train_ames$SalePrice))^2)
R2_test <- 1 - SSE_test / SST_test

cat("In-sample R²:", round(R2_train, 4), "\n")
cat("Out-of-sample R²:", round(R2_test, 4), "\n")
```
Selected mtry value: 36 
In-sample R²: 0.9806 
Out-of-sample R²: 0.872 

### Part f
Out of the four models constructed, I would recommend my model from e - a random forest model with 80 trees, a nodesize of 25, and selected mtry value of 36. I believe this is the most robust model compared to the other linear regression and CART models. Out of all 4 models, this model had the highest out of sample R^2 at 0.872, meaning that 87.2% of the variation in SalePrice from the test dataset could be explained by the model built on 36 variables chosen from the training dataset. The other models we looked at all had lower OOS R^2 at 0.83, 0.69, and 0.79 respectively for a, b, and d. Despite most of those models showing promising in sample R^2 (all above 0.75 and two around 0.90), they didn't perform as well with the test data, meaning that there could be some sort of overfitting occurring that isn't allowing for generalization towards unseen data.  
Also, I prefer my random forest model because it is very flexible compared to the other models. RF algorithms can effectively capture relationships and complex patterns within data by creating numerous decision trees, which can model non-linear boundaries and interactions between features. Specifically, with our mtry feature, at each node of a tree, the algorithm does not consider all available predictors. Instead, it randomly selects mtry number of predictors from the full set of predictors. This makes it less likely to select 2 highly correlated variables because one will be a more important predictor than the other (reducing the rmse more.) Unlike linear models that assume a straightforward relationship, the ensemble nature of a random forest allows it to learn intricate patterns that might be difficult for other algorithms to detect, making it a powerful and flexible tool for many machine learning tasks. The one thing that's not great about the RF is that it isn't interpretable. While individual trees are interpretable, the fact that we have a forest of trees with aggregate decision-making is difficult to follow. However, we can make plots and use techniques like feature importance to gain insights into the model's behavior and identify key drivers of its predictions. 


